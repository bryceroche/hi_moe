{"id":"hi_moe-0n5","title":"Continuous learning / manifold surface","description":"System should move smoothly through representational space rather than discrete jumps. Smooth state transitions, incremental updates.","status":"open","priority":1,"issue_type":"feature","created_at":"2025-12-23T13:21:31.647017-07:00","updated_at":"2025-12-23T13:21:31.647017-07:00"}
{"id":"hi_moe-0qv","title":"Baseline comparison: single model vs hierarchy","description":"Measure coordination value vs capability: Need baseline using single model with no hierarchy to prove the complexity pays for itself. Run same problem set through both, compare success rate and efficiency.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-26T03:09:55.446687Z","updated_at":"2025-12-26T03:10:27.800072Z","dependencies":[{"issue_id":"hi_moe-0qv","depends_on_id":"hi_moe-ld8","type":"blocks","created_at":"2025-12-26T03:10:27.800072Z","created_by":"claude"}]}
{"id":"hi_moe-1ke","title":"Valence signals (feelings) for decisions","description":"Implement simple good/bad scalar attached to outcomes. Provides gradient for the system to follow - 'that worked, do more of that.' Informs routing decisions and routine caching.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-23T13:20:34.630229-07:00","updated_at":"2025-12-23T19:28:13.24811-07:00","closed_at":"2025-12-23T19:28:13.24811-07:00","dependencies":[{"issue_id":"hi_moe-1ke","depends_on_id":"hi_moe-5o2","type":"blocks","created_at":"2025-12-23T13:23:27.45638-07:00","created_by":"jinkang"}]}
{"id":"hi_moe-2tx","title":"State continuity (context vector evolution)","description":"Beads state object maintains compressed 'context vector' that evolves smoothly across tasks rather than resetting. Gives system sense of trajectory.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-23T13:21:51.633233-07:00","updated_at":"2025-12-23T13:21:51.633233-07:00","dependencies":[{"issue_id":"hi_moe-2tx","depends_on_id":"hi_moe-0n5","type":"blocks","created_at":"2025-12-23T13:24:09.017515-07:00","created_by":"jinkang"}]}
{"id":"hi_moe-2ze","title":"DAG support for parallel task execution","description":"Deferred post-v0.1: Support task dependency graphs with parallel execution branches. Requires: parallel specialist invocation, dependency resolution, result aggregation from branches. Blocked by proving linear sequence works first.","status":"open","priority":3,"issue_type":"feature","created_at":"2025-12-26T03:18:20.660493Z","updated_at":"2025-12-26T03:18:49.158216Z","dependencies":[{"issue_id":"hi_moe-2ze","depends_on_id":"hi_moe-d87","type":"blocks","created_at":"2025-12-26T03:18:49.158216Z","created_by":"claude"}]}
{"id":"hi_moe-3ie","title":"Novelty-weighted memory \u0026 forgetting","description":"Implement attention allocation via novelty weighting. Events within confidence bounds get exponentially decayed. Surprises retained longer. Forgetting routine events is a feature.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-23T13:20:40.911195-07:00","updated_at":"2025-12-23T19:28:13.248941-07:00","closed_at":"2025-12-23T19:28:13.248941-07:00","dependencies":[{"issue_id":"hi_moe-3ie","depends_on_id":"hi_moe-5o2","type":"blocks","created_at":"2025-12-23T13:23:32.89293-07:00","created_by":"jinkang"}]}
{"id":"hi_moe-3ml","title":"Self-reflection \u0026 architecture visibility","description":"Enable the system to model itself. Allow introspection into its own capabilities, bottlenecks, and state.","status":"open","priority":1,"issue_type":"feature","created_at":"2025-12-23T13:20:57.830897-07:00","updated_at":"2025-12-23T13:20:57.830897-07:00"}
{"id":"hi_moe-3oq","title":"Abstract Architect tier implementation","description":"First tier of hierarchy - sets strategic goals. Uses frozen Qwen QwQ-32B base. Communicates through structured handoffs: delegating down, receiving outcomes up.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-23T13:22:15.938819-07:00","updated_at":"2025-12-23T14:43:39.689623-07:00","closed_at":"2025-12-23T14:43:39.689623-07:00"}
{"id":"hi_moe-3qj","title":"Code execution sandbox (CodeRunner)","description":"Fleet generates code but needs to actually execute/test it. Implement CodeRunner component with Docker isolation for safe code execution. Required for competitive programming validation.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T14:16:27.312876-07:00","updated_at":"2025-12-23T14:53:40.168529-07:00","closed_at":"2025-12-23T14:53:40.168529-07:00"}
{"id":"hi_moe-46g","title":"LoRA strategy: train only when proven necessary","description":"Revised strategy: skip LoRAs entirely for v0.1. Benefits: validate coordination architecture first, skip training/loading complexity, defer S-LoRA integration, faster iteration. Decision: train or download LoRAs only when evidence shows base model is not good enough for a specific tier. Check HuggingFace for existing Qwen/QwQ adapters (but adapters are model-specific).","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-26T03:37:53.239233Z","updated_at":"2025-12-26T03:38:12.826153Z","dependencies":[{"issue_id":"hi_moe-46g","depends_on_id":"hi_moe-rj4","type":"blocks","created_at":"2025-12-26T03:38:12.826153Z","created_by":"claude"}]}
{"id":"hi_moe-4dy","title":"Dispatcher structured output via prompt enforcement","description":"Minimal schema: {\"steps\": [{\"description\": \"string\", \"specialist\": \"python|math|general\"}]}. Enforce via prompt engineering + validation + single retry. QwQ should achieve ~90%+ parse success. Defer vLLM guided generation until parsing failures become a real problem.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-26T03:18:20.660493Z","updated_at":"2025-12-26T03:58:29.763179Z","closed_at":"2025-12-26T03:58:29.763156Z"}
{"id":"hi_moe-5ip","title":"Tool usage tracking \u0026 analytics","description":"Track usage of all tools - what's used vs unused. Use analytics to inform future tool construction. Problems can identify missing tools.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-23T13:22:08.957477-07:00","updated_at":"2025-12-23T13:22:08.957477-07:00"}
{"id":"hi_moe-5o2","title":"Progress Monitor (Fourth Tier)","description":"Create a meta-layer above Abstract Architect that tracks whether the system is making progress, not just planning/executing. Home for progress tracking, surprise detection, and valence signals.","status":"closed","priority":3,"issue_type":"feature","created_at":"2025-12-23T13:20:21.527607-07:00","updated_at":"2025-12-23T19:25:51.02528-07:00","closed_at":"2025-12-23T19:25:51.02528-07:00","dependencies":[{"issue_id":"hi_moe-5o2","depends_on_id":"hi_moe-3oq","type":"blocks","created_at":"2025-12-23T13:24:15.743374-07:00","created_by":"jinkang"}]}
{"id":"hi_moe-6ik","title":"Add code execution sandbox for validation","description":"Integrate a sandboxed code execution environment (e.g., Modal sandbox or RestrictedPython) to validate generated code against test cases before returning results.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-26T15:37:08.741438-08:00","updated_at":"2025-12-26T15:37:08.741438-08:00"}
{"id":"hi_moe-6x1","title":"Define BeadsClient interface","description":"Specs reference BeadsClient but interface is undefined. Define the client API for get/set/append operations, or document how to use existing beads CLI from Python.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-23T14:16:33.994591-07:00","updated_at":"2025-12-23T19:14:14.136763-07:00","closed_at":"2025-12-23T19:14:14.136763-07:00"}
{"id":"hi_moe-756","title":"Model selection optimization per tier","description":"Deferred optimization: QwQ reasoning overhead is helpful for Architect (strategy) and Dispatcher (routing analysis) but wasteful for Fleet (just execute). Consider Qwen2.5-Instruct for Fleet as faster alternative. v0.1: use QwQ everywhere for simplicity, optimize later if reasoning overhead becomes a bottleneck.","status":"open","priority":3,"issue_type":"feature","created_at":"2025-12-26T03:37:53.239233Z","updated_at":"2025-12-26T03:38:12.826153Z","dependencies":[{"issue_id":"hi_moe-756","depends_on_id":"hi_moe-wvi","type":"blocks","created_at":"2025-12-26T03:38:12.826153Z","created_by":"claude"}]}
{"id":"hi_moe-7af","title":"Fix datetime.utcnow() deprecation warnings","description":"Replace datetime.utcnow() with datetime.now(datetime.UTC) in trajectory_logger.py, tiers.py, and runner.py. Python 3.12+ deprecation.","status":"open","priority":2,"issue_type":"bug","created_at":"2025-12-26T15:37:21.742785-08:00","updated_at":"2025-12-26T15:37:21.742785-08:00"}
{"id":"hi_moe-7b5","title":"Soft routing (weighted specialist contributions)","description":"Instead of hard routing to single specialist, weight contributions from multiple specialists based on task embedding similarity. Routing LoRA learns soft weights.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-23T13:21:38.899476-07:00","updated_at":"2025-12-23T13:21:38.899476-07:00","dependencies":[{"issue_id":"hi_moe-7b5","depends_on_id":"hi_moe-0n5","type":"blocks","created_at":"2025-12-23T13:23:58.089804-07:00","created_by":"jinkang"}]}
{"id":"hi_moe-836","title":"Test trained python-test adapter against eval benchmarks","description":"","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-26T15:36:49.048581-08:00","updated_at":"2025-12-26T15:36:49.048581-08:00"}
{"id":"hi_moe-881","title":"Strategy versioning \u0026 routine saving","description":"When system discovers successful approach to a problem class, snapshot that routing pattern + LoRA combination as named 'routine'. Retrieve and replay on similar problems.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-23T13:21:10.860445-07:00","updated_at":"2025-12-23T13:21:10.860445-07:00","dependencies":[{"issue_id":"hi_moe-881","depends_on_id":"hi_moe-3ml","type":"blocks","created_at":"2025-12-23T13:23:45.729618-07:00","created_by":"jinkang"}]}
{"id":"hi_moe-8ig","title":"vLLM guided generation for structured output","description":"Deferred: Use vLLM JSON schema enforcement for dispatcher output instead of prompt-based enforcement. Add only if prompt-based parsing failures exceed ~10%.","status":"open","priority":4,"issue_type":"feature","created_at":"2025-12-26T03:18:20.660493Z","updated_at":"2025-12-26T03:18:49.158216Z","dependencies":[{"issue_id":"hi_moe-8ig","depends_on_id":"hi_moe-4dy","type":"blocks","created_at":"2025-12-26T03:18:49.158216Z","created_by":"claude"}]}
{"id":"hi_moe-9km","title":"Mode collapse prevention in specialist training","description":"From Gemini feedback: Include handoff protocol examples in training data to prevent mode collapse during specialist LoRA training. Data quality matters more than training technique.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-26T03:09:55.446687Z","updated_at":"2025-12-26T03:09:55.446687Z"}
{"id":"hi_moe-9wm","title":"Implement surprise detector for Progress Monitor","description":"","status":"open","priority":4,"issue_type":"feature","created_at":"2025-12-23T20:32:42.632007-07:00","updated_at":"2025-12-23T20:32:42.632007-07:00"}
{"id":"hi_moe-a1p","title":"Routine compression into skills","description":"Abstract successful repeated patterns into higher-level 'skills'. Individual instances can then be dropped. Saves useful routines for later reuse.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-23T13:21:17.462915-07:00","updated_at":"2025-12-23T13:21:17.462915-07:00","dependencies":[{"issue_id":"hi_moe-a1p","depends_on_id":"hi_moe-3ml","type":"blocks","created_at":"2025-12-23T13:23:51.152722-07:00","created_by":"jinkang"}]}
{"id":"hi_moe-a4w","title":"Tiered retry logic with escalation","description":"Implement retry logic per tier: Fleet (2-3 retries, same specialist with error context), Dispatcher (1-2 retries, try different specialist), Architect (1 retry, revise plan with failure summary), Top-level (1 retry, give up and log everything). Termination: success/max retries/explicit give up.","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-26T03:09:55.446687Z","updated_at":"2025-12-26T03:09:55.446687Z"}
{"id":"hi_moe-bfi","title":"S-LoRA + vLLM integration for hot-swap","description":"Implement unified paging via S-LoRA and vLLM to enable running 12+ specialists concurrently without latency spikes. Solves the hot-swap problem.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T13:22:53.363402-07:00","updated_at":"2025-12-23T13:45:03.375705-07:00","closed_at":"2025-12-23T13:45:03.375705-07:00"}
{"id":"hi_moe-bh3","title":"Improve tier routing logic with learned router","description":"","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-26T15:36:52.921802-08:00","updated_at":"2025-12-26T15:36:52.921802-08:00"}
{"id":"hi_moe-ceg","title":"Add multi-turn context to tier system","description":"Allow tasks to maintain context across multiple turns. The architect could remember previous solutions and dispatcher could track specialist performance over time.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-26T15:37:19.92681-08:00","updated_at":"2025-12-26T15:37:19.92681-08:00"}
{"id":"hi_moe-cg2","title":"Harness learning loop (meta-learning)","description":"Decision: Is this a fixed orchestration system, or one that learns/adapts coordination strategies? If adaptive, Progress Monitor provides training signal for harness itself.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-23T13:23:00.182416-07:00","updated_at":"2025-12-23T13:23:00.182416-07:00"}
{"id":"hi_moe-d2j","title":"Add vector trajectory tracking via embeddings","description":"","status":"open","priority":4,"issue_type":"feature","created_at":"2025-12-23T20:32:50.61286-07:00","updated_at":"2025-12-23T20:32:50.61286-07:00"}
{"id":"hi_moe-d87","title":"Linear task sequence execution (v0.1 simplification)","description":"Dispatcher produces ordered list of steps, not a dependency graph. Execute steps sequentially, re-plan only on failure. Defers: parallel execution branches, complex dependency graphs, dynamic graph modification mid-execution.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-26T03:18:20.660493Z","updated_at":"2025-12-26T04:03:52.603555Z","closed_at":"2025-12-26T04:03:52.603511Z"}
{"id":"hi_moe-dip","title":"Train algorithms specialist adapter on CodeContests dataset","description":"","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-26T15:36:51.850291-08:00","updated_at":"2025-12-26T15:36:51.850291-08:00"}
{"id":"hi_moe-e8h","title":"Integrate adapter with vLLM inference server","description":"Once training completes, upload the python-test adapter to the hi-moe-adapters volume and configure vLLM to serve it via dynamic LoRA loading.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-26T15:37:07.511943-08:00","updated_at":"2025-12-26T15:37:07.511943-08:00"}
{"id":"hi_moe-f5d","title":"Implement self-healing with retry on code execution failure","description":"When code fails validation, send error feedback to the specialist for correction. Track retry counts and use escalation logic.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-26T15:37:09.796891-08:00","updated_at":"2025-12-26T15:37:09.796891-08:00"}
{"id":"hi_moe-fh2","title":"Tournament for Architect prompt variants","description":"Instead of hand-tuning Architect management style, run competitions: Create 2-3 prompt variants (terse vs detailed, aggressive vs conservative), run same problem set through each, track success rate/token efficiency/time, keep winners. Generates coordination training signal without complex meta-learning.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-26T03:09:55.446687Z","updated_at":"2025-12-26T03:09:55.446687Z"}
{"id":"hi_moe-ftb","title":"Add debugging and refactoring specialist adapters","description":"Train LoRA adapters for debugging and refactoring specialists. System prompts already exist in tiers.py. Need training datasets for these domains.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-26T15:37:06.573204-08:00","updated_at":"2025-12-26T15:37:06.573204-08:00"}
{"id":"hi_moe-gr7","title":"Math-first vs Python-direct routing decision logging","description":"Let the model decide routing strategy via prompt. Track math-first vs python-direct decisions alongside outcomes. Signals for math-first: optimization, correctness proofs, time complexity tradeoffs, graph theory. Signals for python-direct: implementation-heavy, spec is the algorithm, string manipulation. Free training data for future routing LoRA.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-26T03:18:20.660493Z","updated_at":"2025-12-26T03:18:49.158216Z","dependencies":[{"issue_id":"hi_moe-gr7","depends_on_id":"hi_moe-r8q","type":"blocks","created_at":"2025-12-26T03:18:49.158216Z","created_by":"claude"}]}
{"id":"hi_moe-hzm","title":"Structured handoff protocol between tiers","description":"Define communication protocol: delegating down, reporting outcomes up. Beads keeps abstract tier aware of detailed outcomes - state object all tiers can read.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T13:23:06.839143-07:00","updated_at":"2025-12-23T13:39:50.511681-07:00","closed_at":"2025-12-23T13:39:50.511681-07:00"}
{"id":"hi_moe-ihj","title":"Cloud economics tracking and buy vs rent criteria","description":"Track Modal costs: A100 80GB ~$2.50/hr, estimated ~$100/month for 10hr/week dev. Buy vs rent break-even: ~7,600 hours (~12-18 months heavy use). Hidden costs of buying: electricity, cooling, chassis/CPU/RAM, maintenance, depreciation. Revisit buying if burning $1k+/month consistently.","status":"open","priority":4,"issue_type":"task","created_at":"2025-12-26T03:37:53.239233Z","updated_at":"2025-12-26T03:37:53.239233Z"}
{"id":"hi_moe-irp","title":"Tinybox evaluation: revisit criteria","description":"Document when to revisit tinybox purchase: 1) Architecture proven on Modal, 2) Monthly cloud costs exceed $2-3k, 3) Need dedicated 24/7 training/dev rig. Skip for v0.1 due to: redundant infrastructure, vLLM issues with consumer GPUs without NVLink, S-LoRA less tested on consumer cards.","status":"open","priority":4,"issue_type":"task","created_at":"2025-12-26T03:18:20.660493Z","updated_at":"2025-12-26T03:18:20.660493Z"}
{"id":"hi_moe-iz9","title":"Runner trajectory logging for vLLM calls","description":"Log every vLLM call with: ts, task_id, call_id, tier, specialist, lora, input, output, tokens_in, tokens_out, latency_ms, status. Storage: JSONL append-only, one file per run in runs/ directory. Unlocks: debugging (replay failures), training data (filter successful traces), cost tracking (tokens per task/tier), performance tuning (identify bottlenecks).","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-26T03:37:53.239233Z","updated_at":"2025-12-26T04:13:53.081206Z","closed_at":"2025-12-26T04:13:53.081206Z","dependencies":[{"issue_id":"hi_moe-iz9","depends_on_id":"hi_moe-xv1","type":"blocks","created_at":"2025-12-26T03:38:12.826153Z","created_by":"claude"}]}
{"id":"hi_moe-jho","title":"Specialized Fleet (LoRA adapters)","description":"Third tier - domain-specific execution via LoRA adapters (Python, CUDA, Math, etc.). All share frozen Qwen QwQ-32B base. Hot-swap via S-LoRA + vLLM unified paging for 12+ concurrent specialists.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-23T13:22:28.514148-07:00","updated_at":"2025-12-23T14:11:09.578547-07:00","closed_at":"2025-12-23T14:11:09.578547-07:00"}
{"id":"hi_moe-joy","title":"TaskContext runtime state implementation","description":"Replace BeadsClient for runtime state with simple in-memory TaskContext object. Beads remains for project tracking, but runtime state uses a dict-based class with get/set/append. Upgrade to Redis/SQLite only if scaling problems arise.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-26T03:09:55.446687Z","updated_at":"2025-12-26T04:06:44.287845Z","closed_at":"2025-12-26T04:06:44.287845Z"}
{"id":"hi_moe-k1q","title":"Train initial LoRA adapters (python, math)","description":"Specs assume adapters exist but none are trained yet. Start with python-lora and math-lora using competitive programming data. Validates the specialist concept works.","status":"in_progress","priority":2,"issue_type":"task","created_at":"2025-12-23T14:16:40.218737-07:00","updated_at":"2025-12-26T03:38:12.826153Z","dependencies":[{"issue_id":"hi_moe-k1q","depends_on_id":"hi_moe-rj4","type":"blocks","created_at":"2025-12-26T03:38:12.826153Z","created_by":"claude"}]}
{"id":"hi_moe-kzt","title":"Introspective state object (extend Beads)","description":"Extend Beads to include system state - which LoRAs are loaded, Dispatcher routing accuracy, bottlenecks. Let Architect reason about system capabilities when planning.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-23T13:21:04.420281-07:00","updated_at":"2025-12-23T13:21:04.420281-07:00","dependencies":[{"issue_id":"hi_moe-kzt","depends_on_id":"hi_moe-3ml","type":"blocks","created_at":"2025-12-23T13:23:40.27314-07:00","created_by":"jinkang"}]}
{"id":"hi_moe-ld8","title":"Wire tiers together for integration","description":"Connect all four tiers (Architect → Dispatcher → Fleet → CodeRunner) with TaskContext state sharing. Blocking the stress test. Requires: finished LoRA training, defined OutcomeSchema.","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-26T03:09:55.446687Z","updated_at":"2025-12-26T03:18:49.158216Z","dependencies":[{"issue_id":"hi_moe-ld8","depends_on_id":"hi_moe-k1q","type":"blocks","created_at":"2025-12-26T03:10:27.800072Z","created_by":"claude"},{"issue_id":"hi_moe-ld8","depends_on_id":"hi_moe-qwo","type":"blocks","created_at":"2025-12-26T03:10:27.800072Z","created_by":"claude"},{"issue_id":"hi_moe-ld8","depends_on_id":"hi_moe-4dy","type":"blocks","created_at":"2025-12-26T03:18:49.158216Z","created_by":"claude"},{"issue_id":"hi_moe-ld8","depends_on_id":"hi_moe-d87","type":"blocks","created_at":"2025-12-26T03:18:49.158216Z","created_by":"claude"}]}
{"id":"hi_moe-mut","title":"Competitive programming stress test","description":"First validation: competitive programming problems where coordination can be objectively measured. Proves tiers are actually coordinating effectively.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-23T13:22:46.463915-07:00","updated_at":"2025-12-26T03:10:27.800072Z","dependencies":[{"issue_id":"hi_moe-mut","depends_on_id":"hi_moe-q9q","type":"blocks","created_at":"2025-12-23T14:17:00.377654-07:00","created_by":"jinkang"},{"issue_id":"hi_moe-mut","depends_on_id":"hi_moe-3qj","type":"blocks","created_at":"2025-12-23T14:17:05.807672-07:00","created_by":"jinkang"},{"issue_id":"hi_moe-mut","depends_on_id":"hi_moe-r8q","type":"blocks","created_at":"2025-12-26T03:10:27.800072Z","created_by":"claude"}]}
{"id":"hi_moe-o46","title":"Credit assignment: identify failure source tier","description":"Open question: When tasks fail, how do you know which tier caused it? Need instrumentation and analysis approach to attribute failures to specific tiers for targeted improvement.","status":"open","priority":3,"issue_type":"feature","created_at":"2025-12-26T03:09:55.446687Z","updated_at":"2025-12-26T03:09:55.446687Z"}
{"id":"hi_moe-p6t","title":"Modal deployment for vLLM inference","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T19:44:30.215801-07:00","updated_at":"2025-12-23T19:45:32.427345-07:00","closed_at":"2025-12-23T19:45:32.427345-07:00"}
{"id":"hi_moe-q9q","title":"Routing Dispatcher tier implementation","description":"Second tier - breaks tasks into graphs and assigns to specialists. Hybrid routing: hardcoded rules for obvious mappings, learned Routing LoRA for ambiguous cases. Hardcoded successes bootstrap training data.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-23T13:22:22.168929-07:00","updated_at":"2025-12-23T14:33:09.824576-07:00","closed_at":"2025-12-23T14:33:09.824576-07:00"}
{"id":"hi_moe-qwo","title":"Define OutcomeSchema for Fleet→Architect handoff","description":"","status":"open","priority":1,"issue_type":"feature","created_at":"2025-12-23T20:32:34.131581-07:00","updated_at":"2025-12-23T20:32:34.131581-07:00","dependencies":[{"issue_id":"hi_moe-qwo","depends_on_id":"hi_moe-k1q","type":"blocks","created_at":"2025-12-23T20:33:15.20357-07:00","created_by":"jinkang"}]}
{"id":"hi_moe-r8q","title":"JSONL logging for training data collection","description":"Instrument all tiers now, train later. Log: Architect (goal, delegation, success criteria, revisions), Dispatcher (task, routing decision, rationale, specialist), Fleet (task, specialist, output, result), End-to-end (full trajectory, outcome, tokens, wall time). Format: JSONL append-only, one file per run, every entry has task_id.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-26T03:09:55.446687Z","updated_at":"2025-12-26T04:22:46.38629Z","closed_at":"2025-12-26T04:22:46.386304Z"}
{"id":"hi_moe-rj4","title":"Test base model without LoRAs first","description":"Validate if LoRA adapters are even needed. Run entire hierarchy with just base QwQ-32B + different prompts. Specialists would be prompt variations, not adapters. The test: if base model + prompts cannot pass competitive programming problems, adding LoRAs probably will not save you. If it can, architecture is validated without adapter complexity.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-26T03:37:53.239233Z","updated_at":"2025-12-26T16:07:27.386191-08:00","closed_at":"2025-12-26T16:07:27.386191-08:00","close_reason":"Base model passes 6/6 tests (100%). LoRA adapters not required for basic coding tasks. Architecture validated."}
{"id":"hi_moe-vdx","title":"Train math specialist adapter on GSM8K dataset","description":"","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-26T15:36:50.830827-08:00","updated_at":"2025-12-26T15:36:50.830827-08:00"}
{"id":"hi_moe-vo8","title":"Collect trajectory data for online learning","description":"Use TrajectoryLogger to collect problem-solution pairs from successful executions. Feed this data back into training pipeline for continuous improvement.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-26T15:37:20.844173-08:00","updated_at":"2025-12-26T15:37:20.844173-08:00"}
{"id":"hi_moe-wj4","title":"Incremental LoRA updates (online learning)","description":"Instead of discrete batch training, do online updates where successful executions slightly shift adapter weights. Research continual adapter learning approaches.","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-23T13:21:45.368586-07:00","updated_at":"2025-12-23T13:21:45.368586-07:00","dependencies":[{"issue_id":"hi_moe-wj4","depends_on_id":"hi_moe-0n5","type":"blocks","created_at":"2025-12-23T13:24:03.547252-07:00","created_by":"jinkang"}]}
{"id":"hi_moe-wvi","title":"v0.1 success criteria: end-to-end validation","description":"Run competitive programming problem through full stack: 1) Architect receives problem, 2) Architect delegates to Dispatcher, 3) Dispatcher routes to python specialist, 4) Specialist generates code, 5) CodeRunner executes/validates, 6) Outcome flows back via TaskContext, 7) Architect sees result. No smoothness, no self-reflection. Just: does the hierarchy coordinate?","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-26T03:09:55.446687Z","updated_at":"2025-12-26T03:10:27.800072Z","dependencies":[{"issue_id":"hi_moe-wvi","depends_on_id":"hi_moe-ld8","type":"blocks","created_at":"2025-12-26T03:10:27.800072Z","created_by":"claude"}]}
{"id":"hi_moe-x3j","title":"Add valence field to Beads state","description":"","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-23T20:32:27.003529-07:00","updated_at":"2025-12-23T20:32:27.003529-07:00","dependencies":[{"issue_id":"hi_moe-x3j","depends_on_id":"hi_moe-qwo","type":"blocks","created_at":"2025-12-23T20:33:08.65811-07:00","created_by":"jinkang"}]}
{"id":"hi_moe-xv1","title":"Implement Runner control loop","description":"Python control loop on Modal that orchestrates all tiers. Responsibilities: 1) Receive problem, 2) Call Architect for goal/delegation, 3) Call Dispatcher for step sequence, 4) For each step call appropriate specialist, 5) Run CodeRunner if code generated, 6) Write outcomes to TaskContext, 7) Feed results back up the chain, 8) Handle retries and re-planning on failure.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-26T03:37:53.239233Z","updated_at":"2025-12-26T04:06:44.287845Z","closed_at":"2025-12-26T04:06:44.287845Z","dependencies":[{"issue_id":"hi_moe-xv1","depends_on_id":"hi_moe-4dy","type":"blocks","created_at":"2025-12-26T03:38:12.826153Z","created_by":"claude"},{"issue_id":"hi_moe-xv1","depends_on_id":"hi_moe-d87","type":"blocks","created_at":"2025-12-26T03:38:12.826153Z","created_by":"claude"}]}
{"id":"hi_moe-y97","title":"Confidence intervals \u0026 surprise detection","description":"Implement confidence bounds on expected outcomes. Flag surprising events (failures or successes outside bounds) for deeper analysis. Routine success gets compressed/forgotten.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-23T13:20:27.93588-07:00","updated_at":"2025-12-23T19:28:13.24612-07:00","closed_at":"2025-12-23T19:28:13.24612-07:00","dependencies":[{"issue_id":"hi_moe-y97","depends_on_id":"hi_moe-5o2","type":"blocks","created_at":"2025-12-23T13:23:22.026726-07:00","created_by":"jinkang"}]}
{"id":"hi_moe-z68","title":"State drift monitoring","description":"From Gemini feedback: State drift is a real risk. Monitor whether Architect knows what actually happened vs what it thinks happened. Track divergence between planned outcomes and actual execution results.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-26T03:09:55.446687Z","updated_at":"2025-12-26T03:10:27.800072Z","dependencies":[{"issue_id":"hi_moe-z68","depends_on_id":"hi_moe-ld8","type":"blocks","created_at":"2025-12-26T03:10:27.800072Z","created_by":"claude"}]}
